{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "KEC_DIR = '/mnt/shared/corpora/German.KEC'\n",
    "kec = pd.read_table(KEC_DIR + '/kec.csv')\n",
    "available_speakers = [os.path.splitext(f)[0] for f in os.listdir(KEC_DIR + '/Artikulographie_tsv') if f.endswith('.tsv')]\n",
    "kec_words = kec[['WordID','Speaker','TimeStartWord', 'TimeEndWord', 'Word']]\n",
    "kec_words = kec_words.drop_duplicates(subset=['Word'])\n",
    "kec_words = kec_words.drop(kec_words[kec_words.Word == '<P>'].index) #Pause\n",
    "kec_words = kec_words[kec_words['Speaker'].str.replace('.TextGrid', '').isin(available_speakers)]\n",
    "\n",
    "#kec_word_counter= Counter(kec_words['Word'])\n",
    "#summe = 0\n",
    "#for word, count in kec_word_counter.most_common(30):\n",
    "#    summe += count\n",
    "#    print(f\"{word}: {count} {summe}\")\n",
    "#    kec_words.drop(kec_words[kec_words.Word == word].index, inplace=True)\n",
    "\n",
    "#kec_words.drop(kec_words[(kec_words.TimeEndWord - kec_words.TimeStartWord) < 0.5].index, inplace=True)\n",
    "#kec_words.drop(kec_words[kec_words['Word'].str.len() < 7].index, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erstmal dass was wir eh schon wissen\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "FASTTEXT_EMBEDDINGS = fasttext.load_model('cc.de.300.bin')\n",
    "small_kec_df = pd.DataFrame()\n",
    "small_kec_df['file'] = [kec_words['Speaker'].astype(str).removesuffix('.TextGrid') + kec_words['WordID'].astype(str)]\n",
    "small_kec_df['label'] = kec_words['Word']\n",
    "small_kec_df['vector'] = kec_words['Word'].apply(lambda x: np.double(FASTTEXT_EMBEDDINGS.get_word_vector(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hier basteln wir uns die signale und nehmen die samplerates gleich mit\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "for idx, row in kec_words.iterrows():\n",
    "    wav_path = os.path.join('/mnt/shared/corpora/German.KEC/Wav_processed_no_names', f\"{row['Speaker']}.wav\")\n",
    "    \n",
    "    if os.path.exists(wav_path):\n",
    "        data, samplerate = sf.read(wav_path)\n",
    "        \n",
    "        start_sample = int(row['TimeStartWord'] * samplerate)\n",
    "        end_sample = int(row['TimeEndWord'] * samplerate)\n",
    "        \n",
    "        small_kec_df['target_sig'] = data[start_sample:end_sample]\n",
    "        small_kec_df['target_sr'] = samplerate\n",
    "    else:\n",
    "        Exception(f\"File {wav_path} does not exist\")\n",
    "\n",
    "small_kec_df['len_cp'] = small_kec_df.apply(lambda row: int(2 * np.ceil(44100 / 220 * len(row['Signal']) / row['SampleRate'])), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jetzt schauen wir uns mal die ema daten an\n",
    "import pandas as pd\n",
    "kec_emas = pd.read_table(KEC_DIR + '/Artikulographie_tsv/rec_025_IS_id_049_Art3.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! The *paule* package is still in alpha. To download pretrained weights for the PAULE model, which is defined in `paule.paule.Paule`, you can invoke `paule.util.download_pretrained_weights()`.\n",
      "Version of the VocalTractLab library: \"API 2.6.0quantling Dec 22 2022\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from paule import util\n",
    "import os\n",
    "import fasttext.util\n",
    "import math\n",
    "#fasttext.util.download_model('de', if_exists='ignore')  # English\n",
    "FASTTEXT_EMBEDDINGS = fasttext.load_model('cc.de.300.bin')\n",
    "PATH = '/home/bob/AndreWorkland/articubench'\n",
    "example_tiny = pd.read_pickle(os.path.join(PATH, 'articubench/data/tiny.pkl'))\n",
    "test_small = pd.read_pickle(os.path.join(PATH, 'articubench/data/common_voice_geco_words_test_subset_slim_prot4.pkl'))\n",
    "test_jahalt = pd.read_pickle(os.path.join(PATH, 'articubench/data/ja_halt.pickle'))\n",
    "\n",
    "\n",
    "small_df_jahalt = pd.DataFrame()\n",
    "small_df_jahalt['file'] = [str(i) + '_jahalt.pkl' for i in range(len(test_jahalt))]\n",
    "small_df_jahalt['label'] = test_jahalt['Word']\n",
    "small_df_jahalt['target_semantic_vector'] = test_jahalt['Word'].apply(lambda x: np.double(FASTTEXT_EMBEDDINGS.get_word_vector(x)))\n",
    "small_df_jahalt['target_sig'] = test_jahalt['Signal']\n",
    "small_df_jahalt['target_sr'] = test_jahalt['SampleRate'] #SampleRate is 22050 for ja_halt\n",
    "small_df_jahalt['len_cp'] = test_jahalt.apply(lambda row: int(2 * math.ceil(44100 / 220 * len(row['Signal']) / row['SampleRate'])), axis=1) #TODO: Wielang sind die cps? Hab als quickfix einfach 402 * len(sig_rec) / sig_sr genommen und dann auf n채chste even int geceilt\n",
    "small_df_jahalt['reference_cp'] = None\n",
    "small_df_jahalt['reference_tongue_height'] = None\n",
    "small_df_jahalt['reference_ema'] = None #TODO: EMA Daten einf체gen\n",
    "small_df_jahalt['reference_ema_TT'] = None #TODO: EMA Daten einf체gen\n",
    "small_df_jahalt['reference_ema_TB'] = None #TODO: EMA Daten einf체gen\n",
    "test_jahalt.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df_geco = pd.DataFrame()\n",
    "small_df_geco['file'] = test_small['file_name']\n",
    "small_df_geco['label'] = test_small['label']\n",
    "small_df_geco['target_semantic_vector'] = test_small['vector']\n",
    "small_df_geco['target_sig'] = test_small['wav_rec']\n",
    "small_df_geco['target_sr'] = test_small['sr_rec']\n",
    "small_df_geco['len_cp'] = test_small['cp_norm'].apply(lambda x: len(x))\n",
    "small_df_geco['reference_cp'] = test_small['cp_norm'].apply(lambda x: util.inv_normalize_cp(x)) #hier cp_norm oder inverses cp_norm?\n",
    "small_df_geco['reference_tongue_height'] = None\n",
    "small_df_geco['reference_ema'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#FASTTEXT_EMBEDDINGS = fasttext.load_model('/home/bob/AndreWorkland/articubench/cc.de.300.bin')\n",
    "\n",
    "LABEL_VECTORS = pd.read_pickle('/home/bob/AndreWorkland/articubench/articubench/data/lexical_embedding_vectors.pkl')\n",
    "# Create new entries for \"ja\" and \"halt\"\n",
    "new_entries = pd.DataFrame({\n",
    "    'label': ['ja', 'halt'],\n",
    "    'vector': [np.round(FASTTEXT_EMBEDDINGS.get_word_vector('ja'),4), np.round(FASTTEXT_EMBEDDINGS.get_word_vector('halt'),4)],\n",
    "    'phones': [['j', 'a'], ['h', 'a', 'l', 't']],\n",
    "    'durations': [[0.13, 0.11], [0.0966, 0.063, 0.0766, 0.085]]\n",
    "})\n",
    "\n",
    "# Append the new entries to LABEL_VECTORS\n",
    "LABEL_VECTORS = pd.concat([LABEL_VECTORS, new_entries], ignore_index=True)\n",
    "LABEL_VECTORS.to_pickle('/home/bob/AndreWorkland/articubench/articubench/data/lexical_embedding_vectors2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! The *articubench* package is still in alpha and the package   does not contain all the files you need to execute the functions defined here. To download model weights (around 50 MB) run `articubench.util.download_pretrained_weights()` once.\n",
      "Version of the VocalTractLab library: \"API 2.5.2quantling Apr 26 2022\"\n",
      "WARNING! The *paule* package is still in alpha. To download pretrained weights for the PAULE model, which is defined in `paule.paule.Paule`, you can invoke `paule.util.download_pretrained_weights()`.\n",
      "Version of the VocalTractLab library: \"API 2.6.0quantling Dec 22 2022\"\n",
      "pretrained_models exist already. Skip download. Path is /home/bob/miniconda3/lib/python3.12/site-packages/paule/pretrained_models\n",
      "Version of pretrained weights is \"common voice 20240306\"\n",
      "Version of pretrained weights is \"common voice 20240306\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'small_df_jahalt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimal_example_results_fast.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pfile:\n\u001b[1;32m     10\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump((results), pfile)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtest_paule_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m, in \u001b[0;36mtest_paule_fast\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_paule_fast\u001b[39m():\n\u001b[0;32m----> 7\u001b[0m     results \u001b[38;5;241m=\u001b[39m score(synth_baseline_segment, preloaded_data\u001b[38;5;241m=\u001b[39m\u001b[43msmall_df_jahalt\u001b[49m[:\u001b[38;5;241m10\u001b[39m], tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, subscores\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, return_individual_subscores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                     )\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimal_example_results_fast.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pfile:\n\u001b[1;32m     10\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump((results), pfile)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'small_df_jahalt' is not defined"
     ]
    }
   ],
   "source": [
    "from articubench.score import score\n",
    "from articubench.control_models import synth_paule_fast, synth_baseline_schwa, synth_paule_acoustic_semvec, synth_baseline_segment\n",
    "import pickle\n",
    "\n",
    "def test_paule_fast():\n",
    "\n",
    "    results = score(synth_baseline_segment, preloaded_data=small_df_jahalt[:10], tasks='all', subscores='all', return_individual_subscores=True\n",
    "                    )\n",
    "    with open('minimal_example_results_fast.pkl', 'wb') as pfile:\n",
    "        pickle.dump((results), pfile)\n",
    "\n",
    "test_paule_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
